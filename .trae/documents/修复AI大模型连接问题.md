## 修复AI大模型连接问题

### 问题分析
通过分析代码，我发现AI模型连接问题主要有以下几个原因：

1. **豆包、文心一言、通义千问模型**：这些模型在调用时直接要求必须有代理（`if (!apiBase) throw new Error('Missing API base for xxx')`），而没有提供直连选项，这是连接失败的主要原因。

2. **Kimi和DeepSeek模型**：虽然支持直连和代理两种方式，但可能存在配置问题，如API端点构建、密钥获取等。

3. **错误处理不够详细**：当API调用失败时，返回的错误信息不够详细，导致难以定位问题。

### 修复计划

1. **为豆包、文心一言、通义千问模型添加直连支持**：
   - 修改`callDoubao`、`callWenxin`、`callQwen`方法，使其支持直连模式
   - 添加API密钥获取逻辑，支持从环境变量或localStorage获取
   - 构建正确的直连URL端点

2. **优化Kimi和DeepSeek模型的连接逻辑**：
   - 检查并优化API端点构建逻辑
   - 确保密钥获取和使用正确
   - 完善错误处理

3. **增强错误信息的详细程度**：
   - 在API调用失败时，返回更详细的错误信息
   - 包括HTTP状态码、错误响应内容等

4. **优化连接测试逻辑**：
   - 确保连接测试能准确反映模型的连接状态
   - 返回更有用的测试结果信息

### 具体修改文件

1. **`src/services/llmService.ts`**：
   - 修改`callDoubao`方法，添加直连支持
   - 修改`callWenxin`方法，添加直连支持
   - 修改`callQwen`方法，添加直连支持
   - 优化错误处理，返回更详细的错误信息

2. **`src/components/ModelSelector.tsx`**：
   - 确保各模型的API密钥输入字段正确配置
   - 优化连接测试的UI反馈

### 预期效果

修复后，所有AI大模型都将支持直连和代理两种连接方式，用户可以根据自己的需求选择合适的连接方式。同时，错误信息将更加详细，有助于用户调试和解决连接问题。